{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit (conda)",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "058e3987cdc23ca73703d754270b8ab188760f883a6d01bf232b44b78a59e767"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ECE324: Assignment 2, Part I\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- Activation function $\\in$ {sigmoid, ReLU, Linear}.\n",
    "- Learning rate `alpha`.\n",
    "- Number of `epochs`.\n",
    "- Random seed `rng`.\n",
    "\n",
    "## Steps\n",
    "\n",
    "- [x] Import `traindata.csv`, `trainlabel.csv`, `validdata.csv`, and `validlabel.csv` via `np.loadtxt()`.\n",
    "- [x] Code the neural network function `Neuron` that takes in {input $I$}.\n",
    "    - [x] Initialize weights to random values between 0 and 1.\n",
    "    - [x] Internal parameters: `alpha`, `activation function` (callable object with two functions: `activate` (default __call__) and `grad` to produce gradient), `weights`, and `bias`.\n",
    "    - [x] `grad_desc` funciton that takes in a full epoch of data and computes average $\\frac{\\partial \\text{loss}}{\\partial w_i}$ for each $w_i$ and bias $b$ -> adjusts parameters using $\\alpha$.\n",
    "- [x] Code a `mean_squared_error` function that takes in neuron output `Y` and compares it with true label `L` and calculates $(I-L)^2$.\n",
    "- [x] Meta-function to run a training trial with the specified hyperparameters {$\\alpha$, `epochs`, `activation`, `random seed`} and output the following values for each epoch:\n",
    "    - [x] Training loss.\n",
    "    - [x] Validation loss.\n",
    "    - [x] Training accuracy.\n",
    "    - [x] Validation accuracy.\n",
    "- [x] Function to 'properly plot' the T+V loss vs. epoch on one graph.\n",
    "- [x] Function to 'properly plot' the T+V accuracy vs. epoch on one graph.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# IMPORT BOX #\n",
    "##############\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=== Validating Data ===\nDimensions of training X: \t(200, 9)\nDimensions of training Y: \t(200,)\n\nDimensions of validation X: \t(20, 9)\nDimensions of validation Y: \t(20,)\n"
    }
   ],
   "source": [
    "###################\n",
    "# DATA IMPORT BOX #\n",
    "###################\n",
    "\n",
    "train_X = np.loadtxt('traindata.csv', delimiter=',')\n",
    "train_Y = np.loadtxt('trainlabel.csv', delimiter=',')\n",
    "valid_X = np.loadtxt('validdata.csv', delimiter=',')\n",
    "valid_Y = np.loadtxt('validlabel.csv', delimiter=',')\n",
    "\n",
    "print(\"=== Validating Data ===\")\n",
    "print(\"Dimensions of training X: \\t{}\".format(train_X.shape))\n",
    "print(\"Dimensions of training Y: \\t{}\".format(train_Y.shape))\n",
    "print(\"\")\n",
    "print(\"Dimensions of validation X: \\t{}\".format(valid_X.shape))\n",
    "print(\"Dimensions of validation Y: \\t{}\".format(valid_Y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    def __init__(self, num_inputs, activ_func, rand_seed=0, alpha=0.01):\n",
    "        self.rand_seed = rand_seed\n",
    "        np.random.seed(rand_seed)\n",
    "        self.weights = np.random.random(num_inputs)\n",
    "        self.bias = np.random.random()\n",
    "        self.activ_func = activ_func\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def Z(self, inp):\n",
    "        return inp.dot(self.weights) + self.bias\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        return self.activ_func(inp.dot(self.weights) + self.bias)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_valid, Y_valid, num_epochs):\n",
    "        train_loss_v_epoch = []\n",
    "        valid_loss_v_epoch = []\n",
    "\n",
    "        train_accuracy_v_epoch = []\n",
    "        valid_accuracy_v_epoch = []\n",
    "\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            train_loss = self.grad_desc(X_train, Y_train)\n",
    "            train_loss_v_epoch.append(train_loss)\n",
    "\n",
    "            valid_loss = self.mean_loss(X_valid, Y_valid)\n",
    "            valid_loss_v_epoch.append(valid_loss)\n",
    "\n",
    "            train_acc = self.get_accuracy(X_train, Y_train)\n",
    "            train_accuracy_v_epoch.append(train_acc)\n",
    "\n",
    "            valid_acc = self.get_accuracy(X_valid, Y_valid)\n",
    "            valid_accuracy_v_epoch.append(valid_acc)\n",
    "\n",
    "        return train_loss_v_epoch, valid_loss_v_epoch, train_accuracy_v_epoch, valid_accuracy_v_epoch\n",
    "\n",
    "    \n",
    "    def grad_desc(self, X, Y):\n",
    "        \"\"\"\n",
    "        Function to perform one step of gradient descent on an epoch of data.\n",
    "\n",
    "        1. Calculate $\\frac{\\partial \\text{loss} }{\\partial \\text{parameter} }$ for each parameter w_i and b.\n",
    "            a. d loss_j / d Y[j]\n",
    "            b. d Y[j] / d Z[j] where Z[j] is the unactivated output of the neuron.\n",
    "            c. d Z[j] / d w_i = X[j][i]; d Z[j] / d b = 1\n",
    "\n",
    "            Multiply a*b*c to get d loss_j / d param_i\n",
    "        2. Calculate the average gradient for each parameter.\n",
    "        3. Apply the update rule to each parameter.\n",
    "        4. Return (average_loss, training_accuracy, validation_accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        # Instantiating Gradients, Loss\n",
    "        weight_grads = np.zeros(self.weights.size)\n",
    "        bias_grad = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "        # Iterating through each training example to calculate gradients for parameters (and avg loss)\n",
    "        for j in range(len(Y)):\n",
    "            pred = self(X[j])\n",
    "            loss = (pred - Y[j])**2\n",
    "\n",
    "            # Solving for components to put into chain rule\n",
    "            dldy = 2*(pred - Y[j])\n",
    "            dydz = self.activ_func.grad(self.Z(X[j]))\n",
    "            dzdw = np.copy(X[j])\n",
    "\n",
    "            # Updating averages via back propagation (chain rule)\n",
    "            weight_grads += (dldy*dydz*dzdw)/len(Y)\n",
    "            bias_grad += (dldy*dydz*1)/len(Y)\n",
    "            avg_loss += loss/len(Y)\n",
    "\n",
    "        # Updating parameters based on gradients and learning rate\n",
    "\n",
    "        self.weights -= weight_grads*self.alpha\n",
    "        self.bias -= bias_grad*self.alpha\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def mean_loss(self, X, Y):\n",
    "        pred = self(X)\n",
    "        sq_diff = (pred - Y)**2\n",
    "        return np.mean(sq_diff)\n",
    "    \n",
    "    def get_accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Function to get the accuracy of the model for predicting values of Y based on X.\n",
    "        \"\"\"\n",
    "        one_if_diff = np.abs(np.round(self(X)) - Y)\n",
    "        num_incorrect = sum(one_if_diff)\n",
    "        accuracy = (len(Y)-num_incorrect)/len(Y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_activation(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Linear'\n",
    "        return\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        \"\"\"\n",
    "        Input is a scalar. We apply a linear activation function by returning the initial value.\n",
    "        \"\"\"\n",
    "\n",
    "        return inp \n",
    "    \n",
    "    def grad(self, inp):\n",
    "        \"\"\"\n",
    "        Linear activation function has $y = x$ -> $dy/dx = 1$\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class sigmoid_activation(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Sigmoid'\n",
    "        return\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        \"\"\"\n",
    "        Input is a scalar. We apply a linear activation function by returning the initial value.\n",
    "        \"\"\"\n",
    "\n",
    "        return 1/(1+np.exp(-inp)) \n",
    "    \n",
    "    def grad(self, inp):\n",
    "        \"\"\"\n",
    "        Linear activation function has $y = x$ -> $dy/dx = 1$\n",
    "        \"\"\"\n",
    "        return self(inp)*(1-self(inp))\n",
    "        \n",
    "class ReLU_activation(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "        return\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        \"\"\"\n",
    "        Input is a scalar. We apply a linear activation function by returning the initial value.\n",
    "        \"\"\"\n",
    "\n",
    "        return max(0, inp)\n",
    "    \n",
    "    def grad(self, inp):\n",
    "        \"\"\"\n",
    "        Linear activation function has $y = x$ -> $dy/dx = 1$\n",
    "        \"\"\"\n",
    "        return np.heaviside(inp, 1)\n",
    "        \n",
    "\n",
    "linear = linear_activation()\n",
    "sigmoid = sigmoid_activation()\n",
    "ReLU = ReLU_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_report(train_loss_v_epoch, valid_loss_v_epoch, train_accuracy_v_epoch, valid_accuracy_v_epoch):\n",
    "    plt.plot(train_loss_v_epoch)\n",
    "    plt.plot(valid_loss_v_epoch)\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_accuracy_v_epoch)\n",
    "    plt.plot(valid_accuracy_v_epoch)\n",
    "    plt.title('Training and Validation Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################\n",
    "# TESTING CELL #\n",
    "################\n",
    "def get_report(activation_func, num_epochs=200, alpha=0.01, rand_seed=0, plot=True):\n",
    "    NN = Neuron(len(train_X[0]), activation_func, alpha=alpha, rand_seed=rand_seed)\n",
    "\n",
    "    train_loss_v_epoch, valid_loss_v_epoch, train_accuracy_v_epoch, valid_accuracy_v_epoch = NN.train(train_X, train_Y, valid_X, valid_Y, num_epochs)\n",
    "\n",
    "    print(\"Activation Function: \\t{}\".format(NN.activ_func.name))\n",
    "    print(\"Learning Rate: \\t\\t{}\".format(NN.alpha))\n",
    "    print(\"Number of Epochs: \\t{}\".format(num_epochs))\n",
    "    print(\"Random Seed: \\t\\t{}\".format(NN.rand_seed))\n",
    "\n",
    "    print('=================================')\n",
    "    print(\"Final Training Accuracy: \\t{}\".format(train_accuracy_v_epoch[-1]))\n",
    "    print(\"Final Validation Accuracy: \\t{}\".format(valid_accuracy_v_epoch[-1]))\n",
    "    print(\"Final Training Loss (avg): \\t{}\".format(train_loss_v_epoch[-1]))\n",
    "    print(\"Final Validation Loss (avg): \\t{}\".format(valid_loss_v_epoch[-1]))\n",
    "\n",
    "    if plot:\n",
    "        plot_report(train_loss_v_epoch, valid_loss_v_epoch, train_accuracy_v_epoch, valid_accuracy_v_epoch)\n",
    "\n",
    "    # TODO: Return a dict with the results from the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "linear = linear_activation()\n",
    "sigmoid = sigmoid_activation()\n",
    "ReLU = ReLU_activation()\n",
    "\"\"\"\n",
    "get_report(activation_func, num_epochs=200, alpha=0.01, rand_seed=0, plot=True):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}