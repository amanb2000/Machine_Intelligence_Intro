{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECE324: Assignment 3\n",
        "\n",
        "## 3: Data Pre-Processing and Visualization (29 Points)\n",
        "\n",
        "### 3.1: Initial Data Tasks:\n",
        "- [ ] Take a gander at `adult.csv`.\n",
        "- [ ] Use `pd.read_csv()` to read the `adult.csv` data into `data`.\n",
        "\n",
        "### 3.2: Sanity Checks:\n",
        "- [ ] Print the `.shape` field of the dataframe.\n",
        "- [ ] Print the `.columns` field of the dataframe (column names).\n",
        "- [ ] Print the `.head()` of the dataframe.\n",
        "- [ ] Use the `data['income'].value_counts()` to determine the number of high and low income earners.\n",
        "\n",
        "Questions (Answer in Final PDF):\n",
        "- [ ] How many high income earners are there? Low income?\n",
        "- [ ] Is the dataset balanced? What are some problems with training on an unbalanced dataset?\n",
        "\n",
        "### 3.3: Cleaning:\n",
        "- [ ] Missing values are indicated by the \"?\" string. Figure out **how many missing values** there are.\n",
        "    - [ ] Do this by iterating over the columns and use the `.isin(\"?\").sum()` function.\n",
        "- [ ] Remove any row that has â‰¥1 \"?\" value. Use `[data[data[column != value]]]`\n",
        "- [ ] Print out the shape of the dataset.\n",
        "\n",
        "Questions (Answer in Final PDF): \n",
        "- [ ] How many samples were removed during cleaning? How many are left?\n",
        "- [ ] Is this a reasonable number of rows to throw out?\n",
        "\n",
        "### 3.4: Balancing the Dataset\n",
        "- [ ] Use `DataFrame.sample` function to balance the dataset. \n",
        "    - [ ] Use the `random_state` argument to ensure same results each time.\n",
        "\n",
        "### 3.5: Visualization and Understanding\n",
        "- [ ] Use the `.describe()` function in the DataFrame class to determine statistics on data.\n",
        "- [ ] Use `verbose_print` method. Ensure the following information is there.\n",
        "    - [ ] Count (number of samples with non-null values)\n",
        "    - [ ] Mean\n",
        "    - [ ] Standard deviation\n",
        "    - [ ] Minimum\n",
        "    - [ ] Lowest 25% of the field.\n",
        "- [ ] Print the number of times each value of the **categorical** features occur in the dataset.\n",
        "    - [ ] Use `pie_chart` from `util.py` to visualize the first 3 categorical features using pie charts.\n",
        "    - [ ] Include these plots in your final report.\n",
        "- [ ] Use the `binary_bar_chart` method to plot the binary bar graph for the first 3 categorical features.\n",
        "    - [ ] Include charts in the report.\n",
        "\n",
        "Questions (Answer in Final PDF):\n",
        "- [ ] What is the minimum age of individuals? Minimum number of hours worked per week in the entire dataset?\n",
        "- [ ] Are certain groups over or under-represented? Do you expect the results traing on the dataset to generalize well to different races?\n",
        "- [ ] What other biases in the dataset can you find? (2 points).\n",
        "---\n",
        "- [ ] List the top three features useful for distinguishing between high and low salary earners.\n",
        "- [ ] How likely would someone who has a high school-level of education be to earn above 50K? How about with Bachelors?\n",
        "\n",
        "### 3.6: Pre-Processing\n",
        "Continuous values should be normalized -> mean = 0, standard deviation = 1. Categorical features should be encoded as 1-hot vectors.\n",
        "\n",
        "- [ ] Extract continuous features into a separate variable.\n",
        "    - [ ] Subtract the average (`.mean()`) and divide by the standard deviation (`.std()`).\n",
        "    - [ ] Return numpy representation using `.values`.\n",
        "- [ ] Use `LabelEncoder` class from sklearn to turn categorical features into integers.\n",
        "    - [ ] Use `OneHotEncoder` class from sklearn to convert integers into 1-hot vectors.\n",
        "    - [ ] Call `fit_transform` in `LabelEncoder` to convert (back to?) integer representation. \n",
        "    - [ ] Be sure to include the \"income\" column!\n",
        "- [ ] Extract the `income` column and store it as a separate variable (numpy array).\n",
        "    - [ ] Do not convert to 1-hot.\n",
        "    - [ ] Remove \"income\" field from feature DataFrame -> separate features from the label.\n",
        "    - [ ] use OneHotEncoder class to convert each categorical feature from integer to one-hot.\n",
        "    - [ ] Stitch the categorical and continuous features back together.\n",
        "\n",
        "Questions (Answer in Final PDF):\n",
        "- [ ] What are some disadvantages of using an integer representation for categorical data?\n",
        "- [ ] What are some disadvantages of using un-normalized continuous data?\n",
        "- [ ] *Bonus*: Create a separate dataset where continuous features are un-normalized and categorical features are repesented as integers. Compare the performance of the NN versus the one created above. Report any differences in the report. \n",
        "\n",
        "### 3.7: Training-Validation Split\n",
        "- [ ] Use `train_test_split` function from sklearn to separate training and validation sets.\n",
        "    - [ ] `test_size` = percentage of data used for testing portion (0 => all data in training). \n",
        "    - [ ] Set `test_size=0.2`. \n",
        "    - [ ] Ensure you use a specified random seed for reproducibility.\n",
        "\n",
        "## 4: Model Training (18 Points)\n",
        "\n",
        "Stochastic gradient descent: Selects random subset of data of fixed size (`mini-batch` or `batch` size) to perform gradient descent + parameter change. Epoch occurs once all mini-batches are used once. \n",
        "\n",
        "### 4.1: DataSet\n",
        "- [ ] Use PyTorch `DataLoader` to manage batch sampling in training loop.\n",
        "    - [ ] Define dataset class extending PyTorch `data.Dataset`.\n",
        "    - [ ] Refer to [this tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
        "    - [ ] Code has been started in `dataset.py`. Complete the implementation for `AdultDataset` class.\n",
        "\n",
        "### 4.2: DataLoader\n",
        "- [ ] Fill in `load_data()` from `main.py`.\n",
        "    - [ ] Instantiate two `AdultDataset` classes -- one for training, one for validation.\n",
        "    - [ ] Create two instances of `DataLoader` class with training + validation datasets.\n",
        "    - [ ] Specify batch size with `batch_size` argument.\n",
        "    - [ ] Specify `shuffle=True` for train loader.\n",
        "\n",
        "Questions (Answer in Final PDF):\n",
        "- [ ] Why is it important to shuffle the data during training? What problem might occur during training if the dataset was collected in a particular order and was not shuffled?\n",
        "\n",
        "### 4.3: Model \n",
        "\n",
        "Model will be a Multi-Layer Perceptron (only linear activation functions) to predict income. Starter code is in `model.py`. \n",
        "\n",
        "- [ ] Create a model with two layers: \n",
        "    - [ ] Linear hidden layer.\n",
        "    - [ ] Linear output layer.\n",
        "    - [ ] First layer should use ReLU activation function.\n",
        "    - [ ] Choose a size for ReLU first layer that is appropriate.\n",
        "- [ ] Define complete architecture in `forward` function.\n",
        "- [ ] Apply `Sigmoid` activation function after the output linear layer.\n",
        "\n",
        "Questions (Answer in Final PDF):\n",
        "- [ ] Justify your choice of the size of the first layer. What should the size of the second (output) layer be?\n",
        "- [ ] Why is the output a probability between 0 and 1? What do 0 and 1 mean if they are the output of the NN? \n",
        "\n",
        "### 4.4: Loss Function and Optimizer\n",
        "- [ ] Define method called `load_model` to instantiate the MLP & optimizer. \n",
        "- [ ] Loss function: Use PyTorch's `MSELoss`.\n",
        "- [ ] Optimizer: `optim.SGD`. \n",
        "- [ ] Input for `load_model`: Learning rate. Guess a good one to default to.\n",
        "\n",
        "### 4.5: Training Loop\n",
        "- [ ] Call `load_data` and `load_model` from main. \n",
        "- [ ] Write a training loop that iterates through total `epochs`.\n",
        "    - [ ] Each `epoch` should iterate over the training loader and perform a gradient step. \n",
        "    - [ ] Use [the same tutorial as before](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for data loader.\n",
        "    - [ ] Use [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for the training process. \n",
        "    - [ ] Zero the gradients at the beginning of each step. \n",
        "- [ ] Inner loop: Print loss and number of correct predictions ever $N=10$ steps (every mini-batch).\n",
        "- [ ] Decision function: If output >0.5, then we predict label 1. Otherwise, we predict 0. \n",
        "\n",
        "### 4.6: Validation\n",
        "- [ ] Create method `evaluate` in `main`.\n",
        "    - [ ] Inputs: Model and validation dataset loader.\n",
        "    - [ ] Output: Accuracy on the training set. \n",
        "- [ ] Print out result of `evaluate` every $N$ training steps (every batch) in the training loop. \n",
        "- [ ] Train the model.\n",
        "    - [ ] Working model = 80% accuracy. \n",
        "    - [ ] Play with hyperparameters until 80% is reached.\n",
        "- [ ] Plot training accuracy and validation accuracy as function of number of `mini-batches`. \n",
        "    - [ ] Training accuracy = averaged accuracy of 10 most recent batches.\n",
        "    - [ ] Include plots in report.\n",
        "    - [ ] Report batch size, MLP hidden size, learning rate.\n",
        "- [ ] Add smothed plots to graphs for better visualization.\n",
        "    - [ ] Use `savgol_filter` from scipy.signal.\n",
        "    - [ ] Small $N$ -> oscillation, consider increasing that too.\n",
        "\n",
        "## 5: Hyperparameters (34 Points)\n",
        "\n",
        "Hyperparameter List:\n",
        "- learning rate\n",
        "- batch size (number of samples in a mini-batch).\n",
        "- activation function of first layer.\n",
        "- number of layers.\n",
        "- loss function.\n",
        "- regularization.\n",
        "\n",
        "We can either use a *random* approach to finding the hyperparameters or a *grid search*. \n",
        "\n",
        "### 5.1: Learning Rate\n",
        "- [ ] Set hidden layer size to 64, batch size to 64.\n",
        "- [ ] Use grid search to find best learning rate.\n",
        "    - [ ] Ensure that there is a separate section at the top to set specific hyperparameters.\n",
        "    - [ ] Vary the learning rate from 1e-3 to 1e+3 (vary by a factor of 10 each time).\n",
        "    - [ ] Re-train model each time. \n",
        "    - [ ] Report highest validation accuracy for each learning rate in a table. \n",
        "    - [ ] Include the table in the report.\n",
        "    - [ ] Plot training accuracy and validation accuracy as a function of the number of steps taken for $\\alpha = 0.01, 1, 100$.\n",
        "    - [ ] Include plot in the final report.\n",
        "\n",
        "Questions (Answered in Final PDF): \n",
        "- [ ] Which learnin rate works the best?\n",
        "- [ ] What happens if the learning rate is too high? Too low?\n",
        "\n",
        "### 5.2: Number of Epochs\n",
        "Good idea to have more rather than fewer. If your validation is still increasing by the end of the run, the training process had too few epochs. Same vice versa. \n",
        "\n",
        "- [ ] Adjust epochs to a 'good setting' for the rest of the assignment.\n",
        "\n",
        "### 5.3: Batch Size\n",
        "Using the best learning rate from the previous section...\n",
        "- [ ] Try batch sizes of 1, 64, and 17932.\n",
        "    - [ ] Make plots of train and validation accuracies versus the number of steps for each.\n",
        "    - [ ] Include the plots in the report. \n",
        "    - [ ] You would need to change $N$ (frequency of recording the training and validation error). \n",
        "    - [ ] Try to reduce the number of epochs for the batch size = 1 case. \n",
        "- [ ] Measure time of the training loop and plot the training and validation accuracy versus time instead of steps.\n",
        "    - [ ] Include plots in the report.\n",
        "\n",
        "Questions (Answered in Final PDF):\n",
        "- [ ] What batch size gives the highest validation accuracy?\n",
        "- [ ] Which batch size is fastest in reaching a high validation accuracy in terms of the number of steps? Which batch size is fastest at reaching maximum validation accuracy in terms of time?\n",
        "- [ ] What happens if batch size is too low? Too high?\n",
        "- [ ] State advantages and disadvantages of small batch size? Large batch size? Generalize a statement about the value of batch size (relative to 1 and the size of the dataset).\n",
        "\n",
        "### 5.4: Under-fitting\n",
        "- [ ] Make MLP have no hidden layers, only one linear layer mapping directly to output (still include Sigmoid).\n",
        "    - [ ] Plot train and validation accuracy versus steps.\n",
        "- [ ] What validation accuracy does the small model achieve? How does this compare to best model trained so far?\n",
        "    - [ ] Is the model underfitting?\n",
        "\n",
        "### 5.5: Over-fitting\n",
        "- [ ] Using the best learning rate and batch size found so far...\n",
        "- [ ] Change MLP to have four layers with hidden layers having size 64. \n",
        "    - [ ] Plot training and validation accuracy versus number of steps.\n",
        "    - [ ] Include plots in report.\n",
        "- [ ] What validation accuracy does the large model achieve? How does it compare to the best model so far? Is the model over fitting?\n",
        "\n",
        "### 5.6: Activation Function\n",
        "- [ ] Take best model trained so far, replace ReLU with tanh THEN sigmoid.\n",
        "    - [ ] Plot train and validation accuracy of all three models on the same graph.\n",
        "    - [ ] Any qualitative differences? Quantitative? \n",
        "    - [ ] Include plots in final report.\n",
        "- [ ] Measure time of each of the training runs with each activation function.\n",
        "    - [ ] Include a table of the times in your report.\n",
        "    - [ ] Is there a difference between activation functions in terms of how long they take?\n",
        "\n",
        "### 5.7: Hyperparameter Search\n",
        "\n",
        "- [ ] Write down the value of the random seed used and hyper parameters values that produced the best results. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello, world!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "execution": {
          "shell.execute_reply": "2020-10-14T03:12:59.307Z",
          "iopub.status.busy": "2020-10-14T03:12:59.292Z",
          "iopub.execute_input": "2020-10-14T03:12:59.295Z",
          "iopub.status.idle": "2020-10-14T03:12:59.304Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-10-14T03:13:02.703Z",
          "iopub.execute_input": "2020-10-14T03:13:02.707Z",
          "iopub.status.idle": "2020-10-14T03:13:02.718Z",
          "shell.execute_reply": "2020-10-14T03:13:02.721Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "execution": {
          "iopub.status.busy": "2020-10-14T03:13:05.071Z",
          "iopub.execute_input": "2020-10-14T03:13:05.075Z",
          "iopub.status.idle": "2020-10-14T03:13:05.083Z",
          "shell.execute_reply": "2020-10-14T03:13:05.087Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "Python 3.8.3 64-bit ('base': conda)",
      "display_name": "Python 3.8.3 64-bit ('base': conda)",
      "metadata": {
        "interpreter": {
          "hash": "058e3987cdc23ca73703d754270b8ab188760f883a6d01bf232b44b78a59e767"
        }
      }
    },
    "nteract": {
      "version": "0.25.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}